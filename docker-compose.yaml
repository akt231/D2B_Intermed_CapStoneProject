version: '2'

networks: 
  appnet:
    driver: bridge

services:
  zookeeper:
    container_name: zookeeper
    hostname: zookeeper
    image: confluentinc/cp-zookeeper:7.3.0
    ports:
      - "2181:2181"
      - "8078:8080"
      - "3181:3181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      #KAFKA_OPTS: "-Dzookeeper.admin.enableServer=false -Dzookeeper.4lw.commands.whitelist=ruok"
    restart: always
    networks: 
      - appnet
    volumes:
      - zk-data:/var/lib/zookeeper/data
      - zk-txn-logs:/var/lib/zookeeper/log

  broker:
    container_name: broker
    hostname: broker
    image: confluentinc/cp-kafka:7.3.0
    ports:
      - "9092:9092"
      - "9101:9101"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092, PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
    restart: always
    working_dir: /home/appuser/kafka/scripts
    networks: 
      - appnet
    volumes:
      - kafka-data:/var/lib/kafka/data
      - kafka-scripts:/home/appuser/kafka/scripts

  producer:
    container_name: producer
    hostname: producer
    build:
      context: .
      dockerfile: ./producer/Dockerfile
    ports:
      - "1488:88"
    #volumes:
    #  - app_producer:/usr/src/app/producer
    working_dir: /usr/src/app/producer
    restart: always
    networks: 
      - appnet

  sparkm:
    container_name: sparkm
    hostname: sparkm
    build:
      context: ./spark
      dockerfile: Dockerfile
    command: bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_WORKLOAD=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks: 
      - appnet
    ports:
      - "9090:8080"
      - "7077:7077"
    #working_dir: /opt/spark-apps
    volumes:
      - sprk_apps:/opt/spark-apps
      - sprk_data:/opt/spark-data
      
  sparkw:
    container_name: sparkw
    hostname: sparkw  
    build:
      context: ./spark
      dockerfile: Dockerfile
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://sparkm:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks: 
      - appnet
    ports:
      - "9091:8080"
      - "7001:7000"
    #working_dir: /opt/spark-apps 
    volumes:
      - sprk_apps:/opt/spark-apps
      - sprk_data:/opt/spark-data
    #networks:
    #  datapipeline:
    #    ipv4_address: 172.18.0.2


volumes:
  zk-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./kafka/logs/zk-data
  zk-txn-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./kafka/logs/zk-txn-logs
  kafka-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./kafka/logs/kafka-data
  kafka-scripts:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./kafka/scripts
  #app_producer:
  #  driver: local
  #  driver_opts:
  #    type: none
  #    o: bind
  #    device: ./producer
  sprk_apps:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./spark/apps
  sprk_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./spark/data